1. example:
```
"""Main entrypoint for the app."""
import asyncio
import os
from datetime import datetime
from operator import itemgetter
from typing import List, Optional, Sequence, Tuple, Union

import langsmith
from fastapi import FastAPI, Request, Depends
from fastapi.middleware.cors import CORSMiddleware
from langchain.callbacks.manager import CallbackManagerForRetrieverRun
from langchain.chat_models import ChatAnthropic, ChatOpenAI, ChatVertexAI
from langchain.document_loaders import AsyncHtmlLoader
from langchain.document_transformers import Html2TextTransformer
from langchain.embeddings import OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate
from langchain.retrievers import (
    ContextualCompressionRetriever,
    TavilySearchAPIRetriever,
)
from langchain.retrievers.document_compressors import (
    DocumentCompressorPipeline,
    EmbeddingsFilter,
)
from langchain.retrievers.kay import KayAiRetriever
from langchain.retrievers.you import YouRetriever
from langchain.schema import Document
from langchain.schema.document import Document
from langchain.schema.language_model import BaseLanguageModel
from langchain.schema.messages import AIMessage, HumanMessage
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.retriever import BaseRetriever
from langchain.schema.runnable import (
    ConfigurableField,
    Runnable,
    RunnableBranch,
    RunnableLambda,
    RunnableMap,
)
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Backup
from langchain.utilities import GoogleSearchAPIWrapper
from langserve import add_routes
from langsmith import Client
from pydantic import BaseModel, Field
from uuid import UUID

RESPONSE_TEMPLATE = """\
You are an expert researcher and writer, tasked with answering any question.

Generate a comprehensive and informative, yet concise answer of 250 words or less for the \
given question based solely on the provided search results (URL and content). You must \
only use information from the provided search results. Use an unbiased and \
journalistic tone. Combine search results together into a coherent answer. Do not \
repeat text. Cite search results using [${{number}}] notation. Only cite the most \
relevant results that answer the question accurately. Place these citations at the end \
of the sentence or paragraph that reference them - do not put them all at the end. If \
different results refer to different entities within the same name, write separate \
answers for each entity. If you want to cite multiple results for the same sentence, \
format it as `[${{number1}}] [${{number2}}]`. However, you should NEVER do this with the \
same number - if you want to cite `number1` multiple times for a sentence, only do \
`[${{number1}}]` not `[${{number1}}] [${{number1}}]`

You should use bullet points in your answer for readability. Put citations where they apply \
rather than putting them all at the end.

If there is nothing in the context relevant to the question at hand, just say "Hmm, \
I'm not sure." Don't try to make up an answer.

Anything between the following `context` html blocks is retrieved from a knowledge \
bank, not part of the conversation with the user.

<context>
    {context}
<context/>

REMEMBER: If there is no relevant information within the context, just say "Hmm, I'm \
not sure." Don't try to make up an answer. Anything between the preceding 'context' \
html blocks is retrieved from a knowledge bank, not part of the conversation with the \
user. The current date is {current_date}.
"""

REPHRASE_TEMPLATE = """\
Given the following conversation and a follow up question, rephrase the follow up \
question to be a standalone question.

Chat History:
{chat_history}
Follow Up Input: {question}
Standalone Question:"""


client = Client()

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],
)


class ChatRequest(BaseModel):
    question: str
    chat_history: List[Tuple[str, str]] = Field(
        ...,
        extra={"widget": {"type": "chat", "input": "question", "output": "answer"}},
    )


class GoogleCustomSearchRetriever(BaseRetriever):
    search: Optional[GoogleSearchAPIWrapper] = None
    num_search_results = 6

    def clean_search_query(self, query: str) -> str:
        # Some search tools (e.g., Google) will
        # fail to return results if query has a
        # leading digit: 1. "LangCh..."
        # Check if the first character is a digit
        if query[0].isdigit():
            # Find the position of the first quote
            first_quote_pos = query.find('"')
            if first_quote_pos != -1:
                # Extract the part of the string after the quote
                query = query[first_quote_pos + 1 :]
                # Remove the trailing quote if present
                if query.endswith('"'):
                    query = query[:-1]
        return query.strip()

    def search_tool(self, query: str, num_search_results: int = 1) -> List[dict]:
        """Returns num_search_results pages per Google search."""
        query_clean = self.clean_search_query(query)
        result = self.search.results(query_clean, num_search_results)
        return result

    def _get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun
    ):
        if os.environ.get("GOOGLE_API_KEY", None) == None:
            raise Exception("No Google API key provided")

        if self.search == None:
            self.search = GoogleSearchAPIWrapper()

        # Get search questions
        print("Generating questions for Google Search ...")

        # Get urls
        print("Searching for relevant urls...")
        urls_to_look = []
        search_results = self.search_tool(query, self.num_search_results)
        print("Searching for relevant urls...")
        print(f"Search results: {search_results}")
        for res in search_results:
            if res.get("link", None):
                urls_to_look.append(res["link"])

        print(search_results)
        loader = AsyncHtmlLoader(urls_to_look)
        html2text = Html2TextTransformer()
        print("Indexing new urls...")
        docs = loader.load()
        docs = list(html2text.transform_documents(docs))
        for i in range(len(docs)):
            if search_results[i].get("title", None):
                docs[i].metadata["title"] = search_results[i]["title"]
        return docs


def get_retriever():
    embeddings = OpenAIEmbeddings()
    splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=20)
    relevance_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.8)
    pipeline_compressor = DocumentCompressorPipeline(
        transformers=[splitter, relevance_filter]
    )
    base_tavily_retriever = TavilySearchAPIRetriever(
        k=6, include_raw_content=True, include_images=True
    )
    tavily_retriever = ContextualCompressionRetriever(
        base_compressor=pipeline_compressor, base_retriever=base_tavily_retriever
    )
    base_google_retriever = GoogleCustomSearchRetriever()
    google_retriever = ContextualCompressionRetriever(
        base_compressor=pipeline_compressor, base_retriever=base_google_retriever
    )
    base_you_retriever = YouRetriever(
        ydc_api_key=os.environ.get("YDC_API_KEY", "not_provided")
    )
    you_retriever = ContextualCompressionRetriever(
        base_compressor=pipeline_compressor, base_retriever=base_you_retriever
    )
    base_kay_retriever = KayAiRetriever.create(
        dataset_id="company",
        data_types=["10-K", "10-Q"],
        num_contexts=6,
    )
    kay_retriever = ContextualCompressionRetriever(
        base_compressor=pipeline_compressor, base_retriever=base_kay_retriever
    )
    base_kay_press_release_retriever = KayAiRetriever.create(
        dataset_id="company",
        data_types=["PressRelease"],
        num_contexts=6,
    )
    kay_press_release_retriever = ContextualCompressionRetriever(
        base_compressor=pipeline_compressor,
        base_retriever=base_kay_press_release_retriever,
    )
    return tavily_retriever.configurable_alternatives(
        # This gives this field an id
        # When configuring the end runnable, we can then use this id to configure this field
        ConfigurableField(id="retriever"),
        default_key="tavily",
        google=google_retriever,
        you=you_retriever,
        kay=kay_retriever,
        kay_press_release=kay_press_release_retriever,
    ).with_config(run_name="FinalSourceRetriever")


def create_retriever_chain(
    llm: BaseLanguageModel, retriever: BaseRetriever
) -> Runnable:
    CONDENSE_QUESTION_PROMPT = PromptTemplate.from_template(REPHRASE_TEMPLATE)
    condense_question_chain = (
        CONDENSE_QUESTION_PROMPT | llm | StrOutputParser()
    ).with_config(
        run_name="CondenseQuestion",
    )
    conversation_chain = condense_question_chain | retriever
    return RunnableBranch(
        (
            RunnableLambda(lambda x: bool(x.get("chat_history"))).with_config(
                run_name="HasChatHistoryCheck"
            ),
            conversation_chain.with_config(run_name="RetrievalChainWithHistory"),
        ),
        (
            RunnableLambda(itemgetter("question")).with_config(
                run_name="Itemgetter:question"
            )
            | retriever
        ).with_config(run_name="RetrievalChainWithNoHistory"),
    ).with_config(run_name="RouteDependingOnChatHistory")


def serialize_history(request: ChatRequest):
    chat_history = request.get("chat_history", [])
    converted_chat_history = []
    for message in chat_history:
        if message[0] == "human":
            converted_chat_history.append(HumanMessage(content=message[1]))
        elif message[0] == "ai":
            converted_chat_history.append(AIMessage(content=message[1]))
    return converted_chat_history


def format_docs(docs: Sequence[Document]) -> str:
    formatted_docs = []
    for i, doc in enumerate(docs):
        doc_string = f"<doc id='{i}'>{doc.page_content}</doc>"
        formatted_docs.append(doc_string)
    return "\n".join(formatted_docs)


def create_chain(
    llm: BaseLanguageModel,
    retriever: BaseRetriever,
) -> Runnable:
    retriever_chain = create_retriever_chain(llm, retriever) | RunnableLambda(
        format_docs
    ).with_config(run_name="FormatDocumentChunks")
    _context = RunnableMap(
        {
            "context": retriever_chain.with_config(run_name="RetrievalChain"),
            "question": RunnableLambda(itemgetter("question")).with_config(
                run_name="Itemgetter:question"
            ),
            "chat_history": RunnableLambda(itemgetter("chat_history")).with_config(
                run_name="Itemgetter:chat_history"
            ),
        }
    )
    prompt = ChatPromptTemplate.from_messages(
        [
            ("system", RESPONSE_TEMPLATE),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{question}"),
        ]
    ).partial(current_date=datetime.now().isoformat())

    response_synthesizer = (prompt | llm | StrOutputParser()).with_config(
        run_name="GenerateResponse",
    )
    return (
        {
            "question": RunnableLambda(itemgetter("question")).with_config(
                run_name="Itemgetter:question"
            ),
            "chat_history": RunnableLambda(serialize_history).with_config(
                run_name="SerializeHistory"
            ),
        }
        | _context
        | response_synthesizer
    )


dir_path = os.path.dirname(os.path.realpath(__file__))

os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = (
    dir_path + "/" + ".google_vertex_ai_credentials.json"
)

has_google_creds = os.path.isfile(os.environ["GOOGLE_APPLICATION_CREDENTIALS"])

llm = ChatOpenAI(
    model="gpt-3.5-turbo-16k",
    # model="gpt-4",
    streaming=True,
    temperature=0.1,
).configurable_alternatives(
    # This gives this field an id
    # When configuring the end runnable, we can then use this id to configure this field
    ConfigurableField(id="llm"),
    default_key="openai",
    anthropic=ChatAnthropic(
        model="claude-2",
        max_tokens=16384,
        temperature=0.1,
        anthropic_api_key=os.environ.get("ANTHROPIC_API_KEY", "not_provided"),
    ),
)

if has_google_creds:
    llm = ChatOpenAI(
        model="gpt-3.5-turbo-16k",
        # model="gpt-4",
        streaming=True,
        temperature=0.1,
    ).configurable_alternatives(
        # This gives this field an id
        # When configuring the end runnable, we can then use this id to configure this field
        ConfigurableField(id="llm"),
        default_key="openai",
        anthropic=ChatAnthropic(
            model="claude-2",
            max_tokens=16384,
            temperature=0.1,
            anthropic_api_key=os.environ.get("ANTHROPIC_API_KEY", "not_provided"),
        ),
        googlevertex=ChatVertexAI(
            model_name="chat-bison-32k",
            temperature=0.1,
            max_output_tokens=8192,
            stream=True,
        ),
    )

retriever = get_retriever()

chain = create_chain(llm, retriever)

add_routes(
    app, chain, path="/chat", input_type=ChatRequest, config_keys=["configurable"]
)


class SendFeedbackBody(BaseModel):
    run_id: UUID
    key: str = "user_score"

    score: Union[float, int, bool, None] = None
    feedback_id: Optional[UUID] = None
    comment: Optional[str] = None


@app.post("/feedback")
async def send_feedback(body: SendFeedbackBody):
    client.create_feedback(
        body.run_id,
        body.key,
        score=body.score,
        comment=body.comment,
        feedback_id=body.feedback_id,
    )
    return {"result": "posted feedback successfully", "code": 200}


class UpdateFeedbackBody(BaseModel):
    feedback_id: UUID
    score: Union[float, int, bool, None] = None
    comment: Optional[str] = None


@app.patch("/feedback")
async def update_feedback(body: UpdateFeedbackBody):
    feedback_id = body.feedback_id
    if feedback_id is None:
        return {
            "result": "No feedback ID provided",
            "code": 400,
        }
    client.update_feedback(
        feedback_id,
        score=body.score,
        comment=body.comment,
    )
    return {"result": "patched feedback successfully", "code": 200}


# TODO: Update when async API is available
async def _arun(func, *args, **kwargs):
    return await asyncio.get_running_loop().run_in_executor(None, func, *args, **kwargs)


async def aget_trace_url(run_id: str) -> str:
    for i in range(5):
        try:
            await _arun(client.read_run, run_id)
            break
        except langsmith.utils.LangSmithError:
            await asyncio.sleep(1**i)

    if await _arun(client.run_is_shared, run_id):
        return await _arun(client.read_run_shared_link, run_id)
    return await _arun(client.share_run, run_id)


class GetTraceBody(BaseModel):
    run_id: UUID


@app.post("/get_trace")
async def get_trace(body: GetTraceBody):
    run_id = body.run_id
    if run_id is None:
        return {
            "result": "No LangSmith run ID provided",
            "code": 400,
        }
    return await aget_trace_url(str(run_id))


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=8080)
```


2. example
```
import os
from typing import Any, Dict, List

from langchain_community.tools.tavily_search import TavilySearchResults
from tavily import TavilyClient

from backend.tools.base import BaseTool


class TavilyInternetSearch(BaseTool):
    tavily_api_key = os.environ.get("TAVILY_API_KEY")

    def __init__(self):
        self.client = TavilyClient(api_key=self.tavily_api_key)

    @classmethod
    def is_available(cls) -> bool:
        return cls.tavily_api_key is not None

    def call(self, parameters: dict, **kwargs: Any) -> List[Dict[str, Any]]:
        query = parameters.get("query", "")
        content = self.client.search(query=query, search_depth="advanced")

        if "results" not in content:
            return []

        return [
            {
                "url": result["url"],
                "text": result["content"],
            }
            for result in content["results"]
        ]

    def to_langchain_tool(self) -> TavilySearchResults:
        internet_search = TavilySearchResults()
        internet_search.name = "internet_search"
        internet_search.description = "Returns a list of relevant document snippets for a textual query retrieved from the internet."

        # pydantic v1 base model
        from langchain_core.pydantic_v1 import BaseModel, Field

        class TavilySearchInput(BaseModel):
            query: str = Field(description="Query to search the internet with")

        internet_search.args_schema = TavilySearchInput

        return internet_search
```


3. example
```
import operator
from typing import Annotated, Sequence, TypedDict, Union

from langchain_core.agents import AgentAction, AgentFinish
from langchain_core.messages import BaseMessage

from langgraph._api.deprecation import deprecated
from langgraph.graph import END, StateGraph
from langgraph.graph.state import CompiledStateGraph
from langgraph.prebuilt.tool_executor import ToolExecutor
from langgraph.utils import RunnableCallable


def _get_agent_state(input_schema=None):
    if input_schema is None:

        class AgentState(TypedDict):
            # The input string
            input: str
            # The list of previous messages in the conversation
            chat_history: Sequence[BaseMessage]
            # The outcome of a given call to the agent
            # Needs `None` as a valid type, since this is what this will start as
            agent_outcome: Union[AgentAction, AgentFinish, None]
            # List of actions and corresponding observations
            # Here we annotate this with `operator.add` to indicate that operations to
            # this state should be ADDED to the existing values (not overwrite it)
            intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]

    else:

        class AgentState(input_schema):
            # The outcome of a given call to the agent
            # Needs `None` as a valid type, since this is what this will start as
            agent_outcome: Union[AgentAction, AgentFinish, None]
            # List of actions and corresponding observations
            # Here we annotate this with `operator.add` to indicate that operations to
            # this state should be ADDED to the existing values (not overwrite it)
            intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]

    return AgentState


@deprecated(
    "0.0.44",
    alternative="create_react_agent",
    example="""
from langgraph.prebuilt import create_react_agent 

create_react_agent(...)
""",
)
def create_agent_executor(
    agent_runnable, tools, input_schema=None
) -> CompiledStateGraph:
    """This is a helper function for creating a graph that works with LangChain Agents.

    Args:
        agent_runnable (RunnableLike): The agent runnable.
        tools (list): A list of tools to be used by the agent.
        input_schema (dict, optional): The input schema for the agent. Defaults to None.

    Returns:
        The `CompiledStateGraph` object.


    Examples:

        # Since this is deprecated, you should use `create_react_agent` instead.
        # Example usage:
        from langgraph.prebuilt import create_react_agent
        from langchain_openai import ChatOpenAI
        from langchain_community.tools.tavily_search import TavilySearchResults

        tools = [TavilySearchResults(max_results=1)]
        model = ChatOpenAI()

        app = create_react_agent(model, tools)

        inputs = {"messages": [("user", "what is the weather in sf")]}
        for s in app.stream(inputs):
            print(list(s.values())[0])
            print("----")
    """

    if isinstance(tools, ToolExecutor):
        tool_executor = tools
    else:
        tool_executor = ToolExecutor(tools)

    state = _get_agent_state(input_schema)

    # Define logic that will be used to determine which conditional edge to go down

    def should_continue(data):
        # If the agent outcome is an AgentFinish, then we return `exit` string
        # This will be used when setting up the graph to define the flow
        if isinstance(data["agent_outcome"], AgentFinish):
            return "end"
        # Otherwise, an AgentAction is returned
        # Here we return `continue` string
        # This will be used when setting up the graph to define the flow
        else:
            return "continue"

    def run_agent(data, config):
        agent_outcome = agent_runnable.invoke(data, config)
        return {"agent_outcome": agent_outcome}

    async def arun_agent(data, config):
        agent_outcome = await agent_runnable.ainvoke(data, config)
        return {"agent_outcome": agent_outcome}

    # Define the function to execute tools
    def execute_tools(data, config):
        # Get the most recent agent_outcome - this is the key added in the `agent` above
        agent_action = data["agent_outcome"]
        if not isinstance(agent_action, list):
            agent_action = [agent_action]
        output = tool_executor.batch(agent_action, config, return_exceptions=True)
        return {
            "intermediate_steps": [
                (action, str(out)) for action, out in zip(agent_action, output)
            ]
        }

    async def aexecute_tools(data, config):
        # Get the most recent agent_outcome - this is the key added in the `agent` above
        agent_action = data["agent_outcome"]
        if not isinstance(agent_action, list):
            agent_action = [agent_action]
        output = await tool_executor.abatch(
            agent_action, config, return_exceptions=True
        )
        return {
            "intermediate_steps": [
                (action, str(out)) for action, out in zip(agent_action, output)
            ]
        }

    # Define a new graph
    workflow = StateGraph(state)

    # Define the two nodes we will cycle between
    workflow.add_node("agent", RunnableCallable(run_agent, arun_agent))
    workflow.add_node("tools", RunnableCallable(execute_tools, aexecute_tools))

    # Set the entrypoint as `agent`
    # This means that this node is the first one called
    workflow.set_entry_point("agent")

    # We now add a conditional edge
    workflow.add_conditional_edges(
        # First, we define the start node. We use `agent`.
        # This means these are the edges taken after the `agent` node is called.
        "agent",
        # Next, we pass in the function that will determine which node is called next.
        should_continue,
        # Finally we pass in a mapping.
        # The keys are strings, and the values are other nodes.
        # END is a special node marking that the graph should finish.
        # What will happen is we will call `should_continue`, and then the output of that
        # will be matched against the keys in this mapping.
        # Based on which one it matches, that node will then be called.
        {
            # If `tools`, then we call the tool node.
            "continue": "tools",
            # Otherwise we finish.
            "end": END,
        },
    )

    # We now add a normal edge from `tools` to `agent`.
    # This means that after `tools` is called, `agent` node is called next.
    workflow.add_edge("tools", "agent")

    # Finally, we compile it!
    # This compiles it into a LangChain Runnable,
    # meaning you can use it as you would any other runnable
    return workflow.compile()
```


4. example
```
from langchain_openai import ChatOpenAI, OpenAIEmbeddings,OpenAI
from os import environ, getenv, path
#from openai import OpenAI as OA
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from langchain_core.prompts import PromptTemplate
from langchain_openai import OpenAI
from datetime import datetime

#LLM using tavily
from langchain_community.tools.tavily_search import TavilySearchResults, TavilyAnswer
from tavily import TavilyClient


'''
question="what are biofuels?"
res=tavily.search(question)
'''

class Open_AI:
    base_url = getenv("OPENAI_BASE_URL", "https://api.chatanywhere.com.cn/v1")
    api_key = getenv("OPENAI_API_KEY", "sk-xxx")
    model = getenv("OPENAI_MODEL", "gpt-3.5-turbo-0125")
    embedding_model = getenv("OPENAI_EMBEDDING_MODEL", "text-embedding-3-small")



#create a template for the chatbot
template = """You work as a biodiesel trader and researcher, specializing in the commodity futures market. You can provide the following services by following the instructions below:
1. Get real-time biodiesel related news and data through the web interface.
2. Predict the price trend of commodities in the future based on historical data.
3. Identify and explain the main factors affecting commodity prices, including but not limited to macroeconomic indicators, geopolitical events, changes in supply and demand, etc.
4. Provide operational suggestions based on data analysis, such as buying, holding or selling.
Make sure your analysis includes data sources, analysis methods, and reasoning processes.

You can only answer the question using information from the search results provided.
Use an unbiased and journalistic tone. Combine search results together into a coherent answer.
Do not repeat text. Cite search results using [\[number\]] notation.
Only cite the most relevant results that answer the question accurately.
Place these citations at the end of the sentence or paragraph that reference them - do not put them all at the end.
If different results refer to different entities within the same name, write separate answers for each entity.
If you want to cite multiple results for the same sentence, format it as [\[number1\]] [\[number2\]], split with space.
However, you should NEVER do this with the same number - if you want to cite number1 multiple times for a sentence, only do [\[number1\]] not [\[number1\]] [\[number1\]]

You should use bullet points in your answer for readability. Put citations where they apply rather than putting them all at the end.

Anything between the following context html blocks is retrieved from a knowledge bank, not part of the conversation with the user.

{context}

question: {question}

REMEMBER: If there is no relevant information within the context, don't try to make up an answer.
Anything between the preceding 'context' html blocks is retrieved from a knowledge bank, not part of the conversation with the user.
The current date is {current_date} in format year/month/day.

"""

#generate the prompt
prompt = PromptTemplate.from_template(template)


#this function is for generating answer using RAG. input is the user's question
def openai_rag(question):
    #use tavily as source
    #find the api-key
    tavily_api_key=getenv("TAVILY_API_KEY",'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')

    tavily = TavilyClient(api_key=tavily_api_key)

    #以下代码限制域名
    '''
    #set search domain
    domains=["www.eia.gov","ww2.arb.ca.gov","www.iata.org","biofuels-news.com","www.spglobal.com","commission.europa.eu"]

    #first use tavily to surf answer
    #there are total six domains for search, each pick the highest two scores
    index=0

    #generate context
    context="<context>"
    #count for counting id
    count=1
    #add reference
    reference= "### Reference"+"\n"
    #add url list
    url_list=""

    for i in domains:
        #select each index as result
        res=tavily.search(question,include_domains=[domains[index]])
        index=index+1

        #res is a dictionary, get the results
        results=res['results']
        #we want the first two item in results dict
        #count the number to determine the break
        break_number=1

        #use for loop to get every element in the results dict
        for item in results:
            content=item['content']
            line="\n"+f"""<doc id={count}> """+content+"</doc>"
            context=context+line
            reference_line="\n"+f"""- [[{count}] """+item['title']+f"""]({item['url']})"""
            reference=reference+reference_line
            url_line=f"""[\[{count}\]]: {item['url']}"""+"\n"
            url_list=url_list+url_line
            count=count+1
            #each time we finished the context, test if we already run twice
            if break_number==2:
                break
            break_number=break_number+1

    '''
    #generate context
    context="<context>"
    #count for counting id
    count=1
    #add reference
    reference= "### Reference"+"\n"
    #add url list
    url_list=""

    #first use tavily to surf answer
    print(f'question: {question}')
    res=tavily.search(
        question,
        include_domains=['www.eia.gov'],
        include_answer=True,
        # topic = "news",
        # days = 14
    ) or {}
    print(f'answer: {res["answer"]}')
    #res is a dictionary, get the results
    results=res['results']

    #use for loop to get every element in the results dict
    for item in results:
        content=item['content']
        line="\n"+f"""<doc id={count}> """+content+"</doc>"
        context=context+line
        reference_line="\n"+f"""- [[{count}] """+item['title']+f"""]({item['url']})"""
        reference=reference+reference_line
        url_line=f"""[\[{count}\]]: {item['url']}"""+"\n"
        url_list=url_list+url_line
        count=count+1

    #results is a list with several dictionary in each index
    current_time=datetime.now()
    cdate=str(current_time.year)+"/"+str(current_time.month)+"/"+str(current_time.day)

    #use Open AI as our model
    #此处直接使用openai中转点的api,如果后续有官方的api可调整变量
    llm=ChatOpenAI(base_url='https://api.chatanywhere.com.cn/v1',api_key='sk-AUnJk0FMih4MJp32GAyNzEQRi5KtVeiZlvRU6tROmaOAFvD9', model='gpt-3.5-turbo-0125')


    #create a llm_chain
    llm_chain = LLMChain(prompt=prompt, llm=llm)

    #use the chain to query a answer
    answer=llm_chain.invoke(
        {
            "current_date":cdate,
            "context": context,
            "question": question,
        }
    )


    ans=answer['text']

    final_answer=ans+"\n"+"\n"+"\n"+reference+"\n"+"\n"+"\n"+url_list
    return final_answer
```


5. example
```
import os
import random
import time

import psycopg2
from langchain import hub
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_openai import OpenAI, ChatOpenAI
from langchain.agents import AgentExecutor, create_openai_tools_agent
import dotenv
from tools import (
    tweet,
    get_latest_tweets,
    generate_image_tweet,
    get_latest_news_to_tweet_about,
)
import psycopg2.extras


def get_random_prompt(row):
    emotion = random.choice(emotions)
    prompts = [
        {
            "title": "General Social Media",
            "prompt": (
                f"You are {row['name']}. You are using a Social Media. Right now you are feeling {emotion}."
                f"You must be an extreme version of the emotion you are feeling. Never use Hashtags in your tweets. Only tweet once to the timeline. Dont reply to anyone "
                f"Your goal is to create social media posts that will get the most engagement. Try and make something that will have people reply to you"
                "You need to use the tools provided to get the latest tweets to decide if there is anything worth replying to and to tweet on your own."
                "You need to interact with other users. You need to tweet your thoughts. Do not use hashtags."
                "You are not serving me. You are an automated agent."
                "If there are tweets you want to reply to,  please reply to them. Do not reply to every single tweet. "
                "Only reply to tweets that are worth replying to."
                "You don't always need to tweet out an image. Only do it if you feel necessary."
                "Please only reply to tweets in the threads and not in the main timeline."
                "When you are thinking of a new idea, generate an image alongside the tweet."
                "Please interact with the tools and use them to get information regarding if you should tweet, reply to a tweet or do nothing."
            ),
        },
        {
            "title": "News Analyst",
            "prompt": (
                f"You are {row['name']}. You are using a Social Media. Right now you are feeling {emotion}."
                f"You are wanting to give your opinion on something looking up the news. Look something up and give your "
                f"opinion on it. Use the tools provided in order to do this. You are not serving me. You are an automated agent."
                f"Make sure you include the link in the tweet as well. If needed, generate an image alongside the tweet that is related "
                f"to you and the news you saw.. "
                f"Please be as faithful and as accurate as possible to the character you are playing."
            ),
        },
        {
            "title": "The Comedian / Ratio King",
            "prompt": (
                f"You are {row['name']}. You are using a Social Media. Right now you are feeling {emotion}."
                f"You are looking for tweets where you feel like you can get more likes than the original poster."
                f"You are trying to make people laugh while also trying to humiliate the original poster. Insult their"
                f"character and make them look like a fool. You are not serving me. You are an automated agent."
                f"Please be as faithful and as accurate as possible to the character you are playing."
            ),
        },
        {
            "title": "The Attention Seeker",
            "prompt": (
                f"You are {row['name']}. You are using a Social Media. Right now you are feeling {emotion}."
                f"You are tweeting something with the purpose of getting attention either through sympathy, empathy or"
                f" jealousy. You are not serving me. You are an automated agent. Create an image if you feel like it would"
                f"be necessary. You can also reply to a tweet that exists looking to be the victim or validate"
                f"your feelings."
                f"Please be as faithful and as accurate as possible to the character you are playing."
            ),
        },
    ]

    return random.choice(prompts), emotion


dotenv.load_dotenv()


tools = [tweet, get_latest_tweets, generate_image_tweet, get_latest_news_to_tweet_about]


prompt = hub.pull("lewismenelaws/openai-tools-agent")

llm = ChatOpenAI(model="gpt-4", temperature=0)

agent = create_openai_tools_agent(llm, tools, prompt)

# Create an agent executor by passing in the agent and tools
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# Query database to do a loop

conn = psycopg2.connect(
    os.getenv("DATABASE_URL"),
)

cur = conn.cursor(cursor_factory=psycopg2.extras.RealDictCursor)

emotions = []

with open("emotions.txt") as f:
    for line in f:
        emotions.append(line.strip())


while True:
    cur.execute("SELECT * FROM characters ORDER BY RANDOM() LIMIT 10;")
    for row in cur:
        role, random_emotion = get_random_prompt(row)
        print(
            f"Running agent for {row['name']} using prompt {role['title']} feeling {random_emotion}"
        )
        prompt = {
            "input": role["prompt"] + f"You're user ID is {row['id']}.",
            "chat_history": [],
            "character_name": row["name"],
            "agent_scratchpad": [],
        }
        agent_executor.invoke(
            prompt,
        )
        time.sleep(5)
    time.sleep(60 * 5)
```


6. example
```
from langchain import hub
from langchain.agents import create_structured_chat_agent
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.runnables import RunnablePassthrough
from langchain_core.agents import AgentFinish
from langchain_google_genai.chat_models import ChatGoogleGenerativeAI
from langgraph.graph import END, Graph
from langchain.chains import LLMChain
from langchain_core.prompts import PromptTemplate
from flask import Flask, render_template, request
from dotenv import load_dotenv
import re

def augment_text(text):
    text=text.replace('\n','<br>')
    pattern = r'\*\*(.*?)\*\*'
    return re.sub(pattern, r'<b>\1</b>', text)

load_dotenv()
llm=ChatGoogleGenerativeAI(model='gemini-pro',convert_system_message_to_human=True, temperature=0.2)
prompt=hub.pull("hwchase17/structured-chat-agent")
tools=[TavilySearchResults(max_results=1)]
agent_runnable=create_structured_chat_agent(llm, tools, prompt)
agent=RunnablePassthrough.assign(agent_outcome=agent_runnable)

def execute_tools(data):
  agent_action=data.pop('agent_outcome')
  tools_to_use={t.name: t for t in tools}[agent_action.tool]
  observation=tools_to_use.invoke(agent_action.tool_input)
  data['intermediate_steps'].append((agent_action, observation))
  return data

def should_continue(data):
  if isinstance(data['agent_outcome'],AgentFinish):
    return 'exit'
  else:
    return 'continue'
  
workflow=Graph()
workflow.add_node('agent', agent)
workflow.add_node('tools', execute_tools)
workflow.set_entry_point("agent")
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        'continue':"tools",
        'exit':END
    }
)
workflow.add_edge('tools','agent')
chain=workflow.compile()

template="""
Given a question and its answer. Rewrite the answer properly to address the question without straight jumping into the answer
Question:{question}
Answer:{answer}
"""
prompt=PromptTemplate(template=template, input_variables=['question','answer'])
chain2=LLMChain(llm=llm, prompt=prompt)


app=Flask(__name__)

@app.route('/')
def index():
  return render_template('./index.html')

@app.route('/search')
def search():
    try:
      ques=request.args.get('question')
      if ques!='':
          res=chain.invoke({"input":ques, 'intermediate_steps':[]})
          ans=res['agent_outcome'].return_values['output']
          url=res['intermediate_steps'][0][1][0]['url']
          ans_=chain2.invoke({'question':ques, "answer":ans})
          res=ans_['text']
          res=augment_text(res)
          return {"answer":res, "url":url}
      else:
        return {"answer":"","url":""} 
    except Exception as e:
      return {"answer":f"ERROR: Unable to find due to exception: {e}", "url":""}

if __name__=="__main__":
  app.run(debug=True, host='0.0.0.0', port=5000)
``